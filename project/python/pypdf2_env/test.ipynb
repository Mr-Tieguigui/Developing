{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats and Dogs\n",
      "OmkarMParkhi1,2 AndreaVedaldi1 AndrewZisserman1 C.V.Jawahar2\n",
      "1DepartmentofEngineeringScience, 2CenterforVisualInformationTechnology,\n",
      "UniversityofOxford, InternationalInstituteofInformationTechnology,\n",
      "UnitedKingdom Hyderabad,India\n",
      "{omkar,vedaldi,az}@robots.ox.ac.uk jawahar@iiit.ac.in\n",
      "Abstract ing different breeds of cats and dogs, a challenging exam-\n",
      "pleofﬁnegrainedobjectcategorizationinlinewiththatof\n",
      "We investigate the ﬁne grained object categorization previous work on ﬂower [15, 32, 33, 39] and animal and\n",
      "problemofdeterminingthebreedofanimalfromanimage. bird species [14, 27, 28, 43] categorization. The difﬁculty\n",
      "To this end we introduce a new annotated dataset of pets, is in the fact that breeds may differ only by a few subtle\n",
      "theOxford-IIIT-Petdataset,covering37differentbreedsof phenotypic details that, due to the highly deformable na-\n",
      "cats and dogs. The visual problem is very challenging as tureofthebodiesofsuchanimals,canbedifﬁculttomea-\n",
      "these animals, particularly cats, are very deformable and sure automatically. Indeed, authors have often focused on\n",
      "therecanbequitesubtledifferencesbetweenthebreeds. cats and dogs as examples of highly deformable objects\n",
      "Wemakeanumberofcontributions:ﬁrst,weintroducea forwhichrecognitionanddetectionisparticularlychalleng-\n",
      "modeltoclassifyapetbreedautomaticallyfromanimage. ing[24,29,34,45].\n",
      "Themodelcombinesshape,capturedbyadeformablepart Beyondthetechnicalinterestofﬁnegrainedcategoriza-\n",
      "modeldetectingthepetface,andappearance,capturedby tion,extractinginformationfromimagesofpetshasaprac-\n",
      "abag-of-wordsmodelthatdescribesthepetfur. Fittingthe tical side too. People devote a lot of attention to their do-\n",
      "modelinvolvesautomaticallysegmentingtheanimalinthe mestic animals, as suggested by the large number of so-\n",
      "image. Second,wecomparetwoclassiﬁcationapproaches: cial networks dedicated to the sharing of images of cats\n",
      "ahierarchicalone,inwhichapetisﬁrstassignedtothecat and dogs: Pet Finder [11], Catster [4], Dogster [5], My\n",
      "ordogfamilyandthentoabreed,andaﬂatone,inwhich Cat Space [9], My Dog Space [10], The International Cat\n",
      "thebreedisobtaineddirectly. Wealsoinvestigateanumber Association [8] and several others [1, 2, 3, 12]. In fact,\n",
      "ofanimalandimageorientatedspatiallayouts. the bulk of the data used in this paper has been extracted\n",
      "These models are very good: they beat all previously from annotated images that users of these social sites post\n",
      "published results on the challenging ASIRRA test (cat vs daily(Sect.2). Itisnotunusualforownerstobelieve(and\n",
      "dogdiscrimination). Whenappliedtothetaskofdiscrimi- post) the incorrect breed for their pet, so having a method\n",
      "natingthe37differentbreedsofpets,themodelsobtainan of automated classiﬁcation could provide a gentle way of\n",
      "averageaccuracyofabout59%,averyencouragingresult alertingthemtosucherrors.\n",
      "consideringthedifﬁcultyoftheproblem. Theﬁrstcontributionofthispaperistheintroductionofa\n",
      "largeannotatedcollectionofimagesof37differentbreeds\n",
      "of cats and dogs (Sect. 2). It includes 12 cat breeds and\n",
      "1.Introduction 25dogbreeds. Thisdataconstitutesthebenchmarkforpet\n",
      "breedclassiﬁcation,and,duetoitsfocusonﬁnegrainedcat-\n",
      "Research on object category recognition has largely fo- egorization,iscomplementarytothestandardobjectrecog-\n",
      "cusedonthediscriminationofwelldistinguishedobjectcat- nition benchmarks. The data, which is publicly available,\n",
      "egories (e.g, airplane vs cat). Most popular international comes with rich annotations: in addition to a breed label,\n",
      "benchmarks(e.g,Caltech-101[22],Caltech-256[26],PAS- eachpethas apixellevelsegmentationanda rectanglelo-\n",
      "CAL VOC [20]) contain a few dozen object classes that, calisingitshead. Asimpleevaluationprotocol,inspiredby\n",
      "forthemostpart,arevisuallydissimilar. Eveninthemuch the PASCAL VOC challenge, is also proposed to enable\n",
      "largerImageNetdatabase[18],categoriesaredeﬁnedbased the comparison of future methods on a common grounds\n",
      "on a high-level ontology and, as such, any visual similar- (Sect.2). Thisdatasetisalsocomplementarytothesubset\n",
      "ity between themis more accidental thansystematic. This ofImageNetusedin[27]fordogs,asitcontainsadditional\n",
      "work concentrates instead on the problem of discriminat- annotations,thoughforfewerbreeds.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# pdfplumber部分：提取PDF文字\n",
    "## 1. 「提取单页pdf文字」\n",
    "## 提取pdf文字\n",
    "import pdfplumber\n",
    "with pdfplumber.open(\"example.pdf\") as pdf:\n",
    "    page01 = pdf.pages[0] #指定页码\n",
    "    text = page01.extract_text()#提取文本\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats and Dogs\n",
      "OmkarMParkhi1,2 AndreaVedaldi1 AndrewZisserman1 C.V.Jawahar2\n",
      "1DepartmentofEngineeringScience, 2CenterforVisualInformationTechnology,\n",
      "UniversityofOxford, InternationalInstituteofInformationTechnology,\n",
      "UnitedKingdom Hyderabad,India\n",
      "{omkar,vedaldi,az}@robots.ox.ac.uk jawahar@iiit.ac.in\n",
      "Abstract ing different breeds of cats and dogs, a challenging exam-\n",
      "pleofﬁnegrainedobjectcategorizationinlinewiththatof\n",
      "We investigate the ﬁne grained object categorization previous work on ﬂower [15, 32, 33, 39] and animal and\n",
      "problemofdeterminingthebreedofanimalfromanimage. bird species [14, 27, 28, 43] categorization. The difﬁculty\n",
      "To this end we introduce a new annotated dataset of pets, is in the fact that breeds may differ only by a few subtle\n",
      "theOxford-IIIT-Petdataset,covering37differentbreedsof phenotypic details that, due to the highly deformable na-\n",
      "cats and dogs. The visual problem is very challenging as tureofthebodiesofsuchanimals,canbedifﬁculttomea-\n",
      "these animals, particularly cats, are very deformable and sure automatically. Indeed, authors have often focused on\n",
      "therecanbequitesubtledifferencesbetweenthebreeds. cats and dogs as examples of highly deformable objects\n",
      "Wemakeanumberofcontributions:ﬁrst,weintroducea forwhichrecognitionanddetectionisparticularlychalleng-\n",
      "modeltoclassifyapetbreedautomaticallyfromanimage. ing[24,29,34,45].\n",
      "Themodelcombinesshape,capturedbyadeformablepart Beyondthetechnicalinterestofﬁnegrainedcategoriza-\n",
      "modeldetectingthepetface,andappearance,capturedby tion,extractinginformationfromimagesofpetshasaprac-\n",
      "abag-of-wordsmodelthatdescribesthepetfur. Fittingthe tical side too. People devote a lot of attention to their do-\n",
      "modelinvolvesautomaticallysegmentingtheanimalinthe mestic animals, as suggested by the large number of so-\n",
      "image. Second,wecomparetwoclassiﬁcationapproaches: cial networks dedicated to the sharing of images of cats\n",
      "ahierarchicalone,inwhichapetisﬁrstassignedtothecat and dogs: Pet Finder [11], Catster [4], Dogster [5], My\n",
      "ordogfamilyandthentoabreed,andaﬂatone,inwhich Cat Space [9], My Dog Space [10], The International Cat\n",
      "thebreedisobtaineddirectly. Wealsoinvestigateanumber Association [8] and several others [1, 2, 3, 12]. In fact,\n",
      "ofanimalandimageorientatedspatiallayouts. the bulk of the data used in this paper has been extracted\n",
      "These models are very good: they beat all previously from annotated images that users of these social sites post\n",
      "published results on the challenging ASIRRA test (cat vs daily(Sect.2). Itisnotunusualforownerstobelieve(and\n",
      "dogdiscrimination). Whenappliedtothetaskofdiscrimi- post) the incorrect breed for their pet, so having a method\n",
      "natingthe37differentbreedsofpets,themodelsobtainan of automated classiﬁcation could provide a gentle way of\n",
      "averageaccuracyofabout59%,averyencouragingresult alertingthemtosucherrors.\n",
      "consideringthedifﬁcultyoftheproblem. Theﬁrstcontributionofthispaperistheintroductionofa\n",
      "largeannotatedcollectionofimagesof37differentbreeds\n",
      "of cats and dogs (Sect. 2). It includes 12 cat breeds and\n",
      "1.Introduction 25dogbreeds. Thisdataconstitutesthebenchmarkforpet\n",
      "breedclassiﬁcation,and,duetoitsfocusonﬁnegrainedcat-\n",
      "Research on object category recognition has largely fo- egorization,iscomplementarytothestandardobjectrecog-\n",
      "cusedonthediscriminationofwelldistinguishedobjectcat- nition benchmarks. The data, which is publicly available,\n",
      "egories (e.g, airplane vs cat). Most popular international comes with rich annotations: in addition to a breed label,\n",
      "benchmarks(e.g,Caltech-101[22],Caltech-256[26],PAS- eachpethas apixellevelsegmentationanda rectanglelo-\n",
      "CAL VOC [20]) contain a few dozen object classes that, calisingitshead. Asimpleevaluationprotocol,inspiredby\n",
      "forthemostpart,arevisuallydissimilar. Eveninthemuch the PASCAL VOC challenge, is also proposed to enable\n",
      "largerImageNetdatabase[18],categoriesaredeﬁnedbased the comparison of future methods on a common grounds\n",
      "on a high-level ontology and, as such, any visual similar- (Sect.2). Thisdatasetisalsocomplementarytothesubset\n",
      "ity between themis more accidental thansystematic. This ofImageNetusedin[27]fordogs,asitcontainsadditional\n",
      "work concentrates instead on the problem of discriminat- annotations,thoughforfewerbreeds.\n",
      "1\n",
      "VOCdata. Thedatasetcontainsabout200imagesforeach\n",
      "breed(whichhavebeensplitrandomlyinto50fortraining,\n",
      "50 for validation, and 100 for testing). A detailed list of\n",
      "breedsisgiveninTab.1,andexampleimagesaregivenin\n",
      "Fig.2. Thedatasetisavailableat[35].\n",
      "Dataset collection. The pet images were downloaded\n",
      "fromCatster[4]andDogster[5],twosocialwebsitesded-\n",
      "icated to the collection and discussion of images of pets,\n",
      "fromFlickr[6]groups, andfromGoogleimages[7]. Peo-\n",
      "ple uploading images to Catster and Dogster provide the\n",
      "Figure 1. Annotations in the Oxford-IIIT Pet data. From left\n",
      "breed information as well, and the Flickr groups are spe-\n",
      "toright: petimage,headboundingbox,andtrimapsegmentation\n",
      "ciﬁc to each breed, which simpliﬁes tagging. For each of\n",
      "(blue: background region; red: ambiguous region; yellow: fore-\n",
      "the 37 breeds, about 2,000 – 2,500 images were down-\n",
      "groundregion).\n",
      "loadedfromthesedatasourcestoformapoolofcandidates\n",
      "for inclusion in the dataset. From this candidate list, im-\n",
      "The second contribution of the paper is a model for pet ages were dropped if any of the following conditions ap-\n",
      "breed discrimination (Sect. 3). The model captures both plied, as judged by the annotators: (i) the image was gray\n",
      "shape(byadeformablepartmodel[23,42]ofthepetface) scale,(ii)anotherimageportrayingthesameanimalexisted\n",
      "andtexture(byabag-of-visual-wordsmodel[16,30,38,44] (which happens frequently in Flickr), (iii) the illumination\n",
      "ofthepetfur).Unfortunately,currentdeformablepartmod- waspoor,(iv)thepetwasnotcenteredintheimage,or(v)\n",
      "els are not sufﬁciently advanced to represent satisfactorily the pet was wearing clothes. The most common problem\n",
      "thehighlydeformablebodiesofcatsanddogs;nevertheless, in all the data sources, however, was found to be errors in\n",
      "they can be used to reliably extract stable and distinctive thebreedlabels. Thuslabelswerereviewedbythehuman\n",
      "componentsofthebody, suchasthepetface. Themethod annotators and ﬁxed whenever possible. When ﬁxing was\n",
      "used in [34] followed from this observation: a cat’s face notpossible,forinstancebecausethepetwasacrossbreed,\n",
      "wasdetectedastheﬁrststageindetectingtheentireanimal. theimagewasdropped. Overall,upto200imagesforeach\n",
      "Herewegofurtherinusingthedetectedheadshapeasapart ofthe37breedswereobtained.\n",
      "of the feature descriptor. Two natural ways of combining\n",
      "theshapeandappearancefeaturesarethenconsideredand Annotations. Eachimageisannotatedwithabreedlabel,\n",
      "compared: aﬂatapproach,inwhichbothfeaturesareused a pixel level segmentation marking the body, and a tight\n",
      "toregressthepet’sfamilyandthebreedsimultaneously,and boundingboxaboutthehead. Thesegmentationisatrimap\n",
      "a hierarchical one, in which the family is determined ﬁrst with regions corresponding to: foreground (the pet body),\n",
      "based on the shape features alone, and then appearance is background, and ambiguous (the pet body boundary and\n",
      "used to predict the breed conditioned on the family. Infer- any accessory such as collars). Fig. 1 shows examples of\n",
      "ringthemodelinanimageinvolvessegmentingtheanimal theseannotations.\n",
      "fromthebackground. Tothisend,weimprovedonourpre-\n",
      "vious method on of segmentation in [34] basing it on the\n",
      "Evaluationprotocol. Threetasksaredeﬁned: petfamily\n",
      "extractionofsuperpixels.\n",
      "classiﬁcation(CatvsDog,atwoclassproblem),breedclas-\n",
      "Themodelisvalidatedexperimentallyonthetaskofdis-\n",
      "siﬁcationgiventhefamily(a12classproblemforcatsand\n",
      "criminating the 37 pet breeds (Sect. 4), obtaining very en-\n",
      "a 25 class problem for dogs), and breed and family classi-\n",
      "couraging results, especially considering the toughness of\n",
      "ﬁcation(a37classproblem). Inallcases,theperformance\n",
      "theproblem. Furthermore, wealsousethemodeltobreak\n",
      "ismeasuredastheaverageper-classclassiﬁcationaccuracy.\n",
      "theASIRRAtestthatusestheabilityofdiscriminatingbe-\n",
      "Thisistheproportionofcorrectlyclassiﬁedimagesforeach\n",
      "tweencatsanddogstotellhumansfrommachines.\n",
      "of the classes and can be computed as the average of the\n",
      "diagonal of the (row normalized) confusion matrix. This\n",
      "2.Datasetsandevaluationmeasures\n",
      "meansthat,forexample,arandomclassiﬁerhasaverageac-\n",
      "curacyof1/2 = 50%forthefamilyclassiﬁcationtask,and\n",
      "2.1.TheOxford-IIITPetdataset\n",
      "of 1/37 ≈ 3% for the breed and family classiﬁcation task.\n",
      "TheOxford-IIITPetdatasetisacollectionof7,349im- Algorithms are trained on the training and validation sub-\n",
      "ages of cats and dogs of 37 different breeds, of which 25 setsandtestedonthetestsubset. Thesplitbetweentraining\n",
      "aredogsand12arecats. Imagesaredividedintotraining, andvalidationisprovidedonlyforconvenience,butcanbe\n",
      "validation,andtestsets,inasimilarmannertothePASCAL disregarded.\n",
      "Breed Training Validation Test Total Breed Training Validation Test Total\n",
      "Abyssinian 50 50 98 198 EnglishSetter 50 50 100 200\n",
      "Bengal 50 50 100 200 GermanShorthaired 50 50 100 200\n",
      "Birman 50 50 100 200 GreatPyrenees 50 50 100 200\n",
      "Bombay 49 47 88 184 Havanese 50 50 100 200\n",
      "BritishShorthair 50 50 100 200 JapaneseChin 50 50 100 200\n",
      "EgyptianMau 47 46 97 190 Keeshond 50 50 99 199\n",
      "MaineCoon 50 50 100 200 Leonberger 50 50 100 200\n",
      "Persian 50 50 100 200 MiniaturePinscher 50 50 100 200\n",
      "Ragdoll 50 50 100 200 Newfoundland 50 46 100 196\n",
      "RussianBlue 50 50 100 200 Pomeranian 50 50 100 200\n",
      "Siamese 50 49 100 199 Pug 50 50 100 200\n",
      "Sphynx 50 50 100 200 SaintBernard 50 50 100 200\n",
      "AmericanBulldog 50 50 100 200 Samoyed 50 50 100 200\n",
      "AmericanPitBullTerrier 50 50 100 200 ScottishTerrier 50 50 99 199\n",
      "BassetHound 50 50 100 200 ShibaInu 50 50 100 200\n",
      "Beagle 50 50 100 200 StaffordshireBullTerrier 50 50 89 189\n",
      "Boxer 50 50 99 199 WheatenTerrier 50 50 100 200\n",
      "Chihuahua 50 50 100 200 YorkshireTerrier 50 50 100 200\n",
      "EnglishCockerSpaniel 50 46 100 196 Total 1846 1834 3669 7349\n",
      "Table1.Oxford-IIITPetdatacomposition.The12catbreedsfollowedbythe25dogbreeds.\n",
      "Abyssinian Bengal Bombay Birman BritishShorthair MaineCoon\n",
      "Persian Egyptian Ragdoll RussianBlue Siamese Sphynx\n",
      "Eng.Setter Boxer Keeshond Havanese BassetHound MiniPinscher\n",
      "Chihuahua GreatPyrenees GermanShorthaired Beagle Staff.BullTerrier Eng.Cocker\n",
      "NewFoundLand Pomeranian Leonberger Am.PitBullTerrier WheatenTerrier JapaneseChin\n",
      "Samoyed ScottishTerrier ShibaInu Pug SaintBernard Am.BullDog\n",
      "Figure2.ExampleimagesfromtheOxford-IIITPetdata.Twoimagesperbreedareshownsidebysidetoillustratethedatavariability.\n",
      "2.2.TheASIRRAdataset chines,andcreatedtheASIRRAtest([19],Fig.3)onthisba-\n",
      "sis. Theassumptionisthat,outofabatchoftwelveimages\n",
      "MicrosoftResearch(MSR)proposedtheproblemofdis-\n",
      "of pets, any machine would predict incorrectly the family\n",
      "criminatingcatsfromdogsasatesttotellhumansfromma-\n",
      "seen by examining the performance of this detector on the\n",
      "cats and dogs in the recent PASCAL VOC 2011 challenge\n",
      "data [20]. The deformable parts detector [23] obtains an\n",
      "Average Precision (AP) of only 31.7% and 22.1% on cats\n",
      "and dogs respectively [20]; by comparison, an easier cat-\n",
      "egory such as bicycle has AP of 54% [20]. However, in\n",
      "thePASCALVOCchallengethetaskistodetectthewhole\n",
      "body of the animal. As in the method of [34], we use the\n",
      "deformable part model to detect certain stable and distinc-\n",
      "tivecomponentsofthebody. Inparticular,theheadannota-\n",
      "tionsincludedintheOxford-IIITPetdataareusedtolearn\n",
      "a deformable part model of the cat faces, and one of the\n",
      "dogfaces([24,29,45]alsofocusonmodellingthefacesof\n",
      "pets). Sect. 4.1 shows that these shape models are in fact\n",
      "verygood.\n",
      "3.2.Appearancemodel\n",
      "Figure3.ExampleimagesfromtheMSRASIRRAdataset.\n",
      "Torepresenttexture,weuseabag-of-words[16]model.\n",
      "Visualwords[38]arecomputeddenselyontheimagebyex-\n",
      "ofatleastoneofthem,whilehumanswouldmakenomis-\n",
      "tractingSIFTdescriptors[31]withastrideof6pixelsand\n",
      "takes. TheASIRRAtestiscurrentlyusedtoprotectanum-\n",
      "at4scales,deﬁnedbysettingthewidthoftheSIFTspatial\n",
      "berofwebsitesfromtheunwantedaccessbyInternetbots.\n",
      "binsto4,6,8,and10pixelsrespectively.TheSIFTfeatures\n",
      "However, the reliability of this test depends on the clas-\n",
      "haveconstantorientation(i.e,theyarenotadaptedtothelo-\n",
      "siﬁcation accuracy α of the classiﬁer implemented by the\n",
      "calimageappearance).TheSIFTdescriptorsarethenquan-\n",
      "bot. For instance, if the classiﬁer has accuracy α = 95%,\n",
      "tizedbasedonavocabularyof4,000visualwords. Thevo-\n",
      "thenthebotfoolstheASIRRAtestroughlyhalfofthetimes\n",
      "cabularyislearnedbyusingk-meansonfeaturesrandomly\n",
      "(α12 ≈54%).\n",
      "sampledfromthetrainingdata.Inordertoobtainadescrip-\n",
      "The complete MSR ASIRRA system is based on a\n",
      "tor for the image, the quantized SIFT features are pooled\n",
      "databaseofseveralmillionsimagesofpets,equallydivided\n",
      "intoaspatialhistogram[30],whichhasdimensionequalto\n",
      "between cats and dogs. Our classiﬁers are tested on the\n",
      "4,000timesthenumberofspatialbins.Histogramsarethen\n",
      "24,990imagesthathavebeenmadeavailabletothepublic l1normalizedandusedinasupportvectormachine(SVM)\n",
      "forresearchandevaluationpurposes. basedontheexponential-χ2kernel[44]forclassiﬁcation.\n",
      "Different variants of the spatial histograms can be ob-\n",
      "tainedbyplacingthespatialbinsincorrespondenceofpar-\n",
      "3.Amodelforbreeddiscrimination\n",
      "ticulargeometricfeaturesofthepet. Theselayoutsarede-\n",
      "The breed of a pet affects its size, shape, fur type and scribednextandinFig.4:\n",
      "color. Sinceitisnotpossibletomeasurethepetsizefrom\n",
      "animagewithoutanabsolutereference,ourmodelfocuses Imagelayout. Thislayoutconsistsofﬁvespatialbinsor-\n",
      "oncapturingthepetshape(Sect.3.1)andtheappearanceof ganizedasa1×1anda2×2grids(Fig.4a)coveringthe\n",
      "its fur (Sect. 3.2). The model also involves automatically entire image area, as in [30]. This results in a 20,000 di-\n",
      "segmentingthepetfromtheimagebackground(Sect.3.3). mensionalfeaturevector.\n",
      "3.1.Shapemodel\n",
      "Image+headlayout. Thislayoutaddstotheimagelayout\n",
      "To represent shape, we use the deformable part model just described a spatial bin in correspondence of the head\n",
      "of[23]. Inthismodel,anobjectisgivenbyarootpartcon- bounding box (as detected by the deformable part model\n",
      "nected with springs to eight smaller parts at a ﬁner scale. of the pet face) as well as one for the complement of this\n",
      "The appearance of each part is represented by a HOG ﬁl- box. Thesetworegionsdonotcontainfurtherspatialsubdi-\n",
      "ter[17],capturingthelocaldistributionoftheimageedges; visions (Fig. 4b). Concatenating the histograms for all the\n",
      "inference(detection)usesdynamicprogrammingtoﬁndthe spatial bins in this layout results in a 28,000 dimensional\n",
      "besttrade-offbetweenmatchingwelleachparttotheimage featurevector.\n",
      "andnotdeformingthespringstoomuch.\n",
      "While powerful, this model is insufﬁcient to represent Image+head+body layout. This layout combines the\n",
      "the ﬂexibility and variability of a pet body. This can be spatialtilesintheimagelayoutwithanadditionalspatialbin\n",
      "Method MeanSegmentationAccuracy\n",
      "Allforeground 45%\n",
      "Parkhietal.[34] 61%\n",
      "(a) Image (b) Image+Head Thispaper 65%\n",
      "Table2.Performanceofsegmentationschemes. Segmentation\n",
      "accuracy computed as intersection over union of segmentation\n",
      "withgroundtruth.\n",
      "(c) Image+Head+Body\n",
      "Dataset MeanClassiﬁcationAccuracy\n",
      "Figure4.Spatialhistogramlayouts. Thethreedifferentspatial Oxford-IIITPetDataset 38.45%\n",
      "layoutsusedforcomputingtheimagedescriptors. Theimagede- UCSD-CaltechBirds 6.91%\n",
      "scriptor in each case is formed by concatenating the histograms Oxford-Flowers102 53.71%\n",
      "computedontheindividualspatialcomponentsofthelayout.The\n",
      "spatialbinsaredenotedbyyellow-blacklines. Table3.Finegrainedclassiﬁcationbaseline.Meanclassiﬁcation\n",
      "accuraciesobtainedonthreedifferentdatasetsusingtheVLFeat-\n",
      "BoWclassiﬁcationcode.\n",
      "in correspondence of the pet head (as for the image+head\n",
      "layout) as well as other spatial bins computed on the fore-\n",
      "4.Experiments\n",
      "groundobjectregionanditscomplement,asdescribednext\n",
      "and in Fig. 4c. The foreground region is obtained either\n",
      "The models are evaluated ﬁrst on the task of discrim-\n",
      "from the automatic segmentation of the pet body or from\n",
      "inating the family of the pet (Sect. 4.1), then on the one\n",
      "the ground-truth segmentation to obtain a best-case base-\n",
      "of discriminating their breed given the family (Sect. 4.2),\n",
      "line. The foreground region is subdivided into ﬁve spatial\n",
      "and ﬁnally discriminating both the family and the breed\n",
      "bins,similartotheimagelayout.Anadditionalbinobtained\n",
      "(Sect. 4.3). For the third task, both hierarchical classiﬁca-\n",
      "from the foreground region with the head region removed\n",
      "tion (i.e, determining ﬁrst the family and then the breed)\n",
      "and no further spatial subdivisions is also used. Concate-\n",
      "and ﬂat classiﬁcation (i.e, determining the family and the\n",
      "nating the histograms for all the spatial bins in this layout\n",
      "breed simultaneously) are evaluated. Training uses the\n",
      "resultsina48,000dimensionalfeaturevector.\n",
      "Oxford-IIIT Pet train and validation data and testing uses\n",
      "theOxford-IIITPettestdata. Alltheseresultsaresumma-\n",
      "3.3.Automaticsegmentation\n",
      "rizedinTab.4andfurtherresultsforpetfamilydiscrimina-\n",
      "Theforeground(pet)andbackgroundregionsneededfor tionontheASIRRAdataarereportedinSect.4.1. Failure\n",
      "computing the appearance descriptors are obtained auto- casesarereportedinFig.7.\n",
      "matically using the grab-cut segmentation technique [36].\n",
      "Initialization of grab-cut segmentations was done using\n",
      "Baseline. InordertocomparethedifﬁcultyoftheOxford-\n",
      "cues from the over-segmentation of an image (i.e, super-\n",
      "IIIT Pet dataset to other Fine Grained Visual Catego-\n",
      "pixels) similar to the method of [15]. In this method, a\n",
      "rization datasets, and also to provide a baseline for our\n",
      "SVM classiﬁer is used to assign superpixels a conﬁdence\n",
      "breedclassiﬁcationtask,wehaverunthepubliclyavailable\n",
      "score. This conﬁdence score is then used to assign super-\n",
      "VLFeat [40] BoW classiﬁcation code over three datasets:\n",
      "pixels to a foreground or background region to initialize\n",
      "Oxford Flowers 102 [33], UCSD-Caltech Birds [14], and\n",
      "thegrab-cutiteration. WeusedBerkeley’sultrametriccolor\n",
      "Oxford-IIITPetdataset(notethatthiscodeisafastersuc-\n",
      "map(UCM)[13]forobtainingthesuperpixels. Eachsuper-\n",
      "cessortotheVGG-MKLpackage[41]usedontheUCSD-\n",
      "pixelwasdescribedbyafeaturevectorcomprisingthecolor\n",
      "CaltechBirdsdatasetin[14]). Thecodeemploysaspatial\n",
      "histogramandSift-BoWhistogramcomputedonit. Super-\n",
      "pyramid[30],butdoesnotusesegmentationorsalientparts.\n",
      "pixelswereassignedascoreusingalinear-SVM[21]which\n",
      "TheresultsaregiveninTable3.\n",
      "was trained on the features computed on the training data.\n",
      "After this initialization, grab-cut was used as in [34]. The 4.1.Petfamilydiscrimination\n",
      "improved initialization achieves segmentation accuracy of\n",
      "This section evaluates the different models on the task\n",
      "65% this improving over our previous method [34] by 4%\n",
      "ofdiscriminatingthefamilyofapet(catVsdogclassiﬁca-\n",
      "andisabout20%betterthansimplychoosingallpixelsas\n",
      "tion).\n",
      "foreground(i.e,assumingthepetforegroundentirelyoccu-\n",
      "piestheimage). (Tab.2). Examplesegmentationsproduced\n",
      "by our method on the Oxford-IIIT Pet data are shown in Shapeonly. Themaximumresponseofthecatfacedetec-\n",
      "Fig.5. tor(Sect.3.1)onanimageisusedasanimage-levelscore\n",
      ". Shape Appearance ClassiﬁcationAccuracy(%)\n",
      "layouttype usinggroundtruth family breed(S.4.2) both(S.4.3)\n",
      "(S.4.1) cat dog hierarchical ﬂat\n",
      "1 (cid:88) – – 94.21 NA NA NA NA\n",
      "2 – Image – 82.56 52.01 40.59 NA 39.64\n",
      "3 – Image+Head – 85.06 60.37 52.10 NA 51.23\n",
      "4 – Image+Head+Body – 87.78 64.27 54.31 NA 54.05\n",
      "5 – Image+Head+Body (cid:88) 88.68 66.12 57.29 NA 56.60\n",
      "6 (cid:88) Image – 94.88 50.27 42.94 42.29 43.30\n",
      "7 (cid:88) Image+Head – 95.07 59.11 54.56 52.78 54.03\n",
      "8 (cid:88) Image+Head+Body – 94.89 63.48 55.68 55.26 56.68\n",
      "9 (cid:88) Image+Head+Body (cid:88) 95.37 66.07 59.18 57.77 59.21\n",
      "Table4.Comparisonbetweendifferentmodels.Thetablecomparesdifferentmodelsonthethreetasksofdiscriminatingthefamily,the\n",
      "breedgiventhefamily,andthebreedandfamilyofthepetsintheOxford-IIITPetdataset(Sect.2). Differentcombinationsoftheshape\n",
      "features(deformablepartmodelofthepetfaces)andofthevariousappearancefeaturesaretested(Sect.3.2,Fig.4).\n",
      "for the cat class. The same is done to obtain a score for Method MeanClass.Accuracy\n",
      "thedogclass. ThenalinearSVMislearnedtodiscriminate Golleetal.[25] 82.7%\n",
      "betweencatsanddogsbasedonthesetwoscores. Theclas- Thispaper(Shapeonly) 92.9%\n",
      "siﬁcationaccuracyofthismodelontheOxford-IIITPettest\n",
      "datais94.21%. Table 5. Performance on ASIRRA Data. Table shows perfor-\n",
      "manceachievedontaskofpetfamilyclassiﬁcationposedbythe\n",
      "ASIRRAchallenge. BestresultsobtainedbyGolle[25]wereob-\n",
      "tained using 10000 images from the data. 8000 for training and\n",
      "Appearanceonly. Spatialhistogramsofvisualwordsare\n",
      "2000fortesting. Ourtestresultsareshownon24990imagesin\n",
      "usedinanon-linearSVMtodiscriminatebetweencatsand\n",
      "theASIRRAdataset.\n",
      "dogs, as detailed in Sect. 3.2. The accuracy depends on\n",
      "the type of spatial histograms considered, which in turn\n",
      "depends on the layout of the spatial bins. On the Oxford- 92.9%,whichcorrespondstoa42%probabilityofbreaking\n",
      "IIIT Pet test data, the image layout obtains an accuracy of thetestinasingletry.Forcomparison,thebestaccuracyre-\n",
      "82.56%;addingheadinformationusingimage+headlayout portedintheliteratureontheASIRRAdatais82.7%[25],\n",
      "yields an accuracy of 85.06%. Using image+head+body which corresponds to just a 9.2% chance of breaking the\n",
      "layoutimprovesaccuracybyafurther2.7%to87.78%. An test. Duetolackofsufﬁcienttrainingdatatotrainappear-\n",
      "improvement of 1% was observed when the ground-truth ance models for ASIRRA data, we did not evaluate these\n",
      "segmentations were used inplace of the segmentations es- modelsonASIRRAdataset.\n",
      "timatedbygrab-cut(Sect.3.2). Thisprogressionindicates\n",
      "4.2.Breeddiscrimination\n",
      "that the more accurate the localization of the pet body, the\n",
      "betteristheclassiﬁcationaccuracy. Thissectionevaluatesthemodelsonthetaskofdiscrimi-\n",
      "natingthedifferentbreedsofcatsanddogsgiventheirfam-\n",
      "ily. This is done by learning a multi-class SVM by using\n",
      "Shapeandappearance. Theappearanceandshapeinfor- the 1-Vs-rest decomposition [37] (this means learning 12\n",
      "mationarecombinedbysummingtheexp-χ2kernelforthe binaryclassiﬁersforcatsand25fordogs).Therelativeper-\n",
      "appearance part (Sect. 3.2) with a linear kernel on the cat formanceofthedifferentmodelsissimilartothatobserved\n",
      "scoresandalinearkernelonthedogscores. Thecombina- for pet family classiﬁcation in Sect. 4.1. The best breed\n",
      "tion boosts the performance by an additional 7% over that classiﬁcation accuracies for cats and dogs are 63.48% and\n",
      "ofusingappearancealone,yieldingapproximately95.37% 55.68%respectively,whichimproveto66.07%and59.18%\n",
      "accuracy(Table4,rows5and9),withallthevariantsofthe whenthegroundtruthsegmentationsareused.\n",
      "appearancemodelperformingsimilarly.\n",
      "4.3.Familyandbreeddiscrimination\n",
      "TheASIRRAdata. TheASIRRAdatadoesnotspecifya Thissectioninvestigatesclassifyingboththefamilyand\n",
      "trainingset, soweusedmodelstrainedontheOxford-IIIT thebreed. Twoapproachesareexplored: hierarchicalclas-\n",
      "Pet data and the ASIRRA data was used only for testing. siﬁcation,inwhichthefamilyisdecidedﬁrstasinSect.4.1,\n",
      "The accuracy of the shape model on the ASIRRA data is andthenthebreedisdecidedasinSect.4.2,andﬂatclassi-\n",
      "Abyssinian  1 35.7%\n",
      "Bengal  2 39.0%\n",
      "Birman  3 77.0%\n",
      "Bombay  4 81.8%\n",
      "British Shorthair  5 69.0%\n",
      "Egyptian Mau  6 71.1%\n",
      "Maine Coon  7 60.0%\n",
      "Persian  8 64.0%\n",
      "Ragdoll  9 51.0%\n",
      "Russian Blue 10 46.0%\n",
      "Siamese 11 70.0%\n",
      "Sphynx 12 82.0%\n",
      "Am. Bulldog 13 52.0%\n",
      "Am. Pit Bull Terrier 14 4.0%\n",
      "Basset Hound 15 62.0%\n",
      "Beagle 16 33.0%\n",
      "Boxer 17 38.4%\n",
      "Chihuahua 18 20.0%\n",
      "Eng. Cocker Spaniel 19 29.0%\n",
      "Eng. Setter 20 43.0%\n",
      "German Shorthaired 21 80.0%\n",
      "Great Pyrenees 22 70.0%\n",
      "Havanese 23 51.0%\n",
      "Japanese Chin 24 82.0%\n",
      "Keeshond 25 75.8%\n",
      "Leonberger 26 53.0%\n",
      "Miniature Pinscher 27 39.0%\n",
      "Newfoundland 28 82.0%\n",
      "Pomeranian 29 28.0%\n",
      "Pug 30 85.0%\n",
      "Saint Bernard 31 59.0%\n",
      "Samoyed 32 91.0%\n",
      "Scottish Terrier 33 66.7%\n",
      "Shiba Inu 34 57.0%\n",
      "Staff. Bull Terrier 35 37.1%\n",
      "Wheaten Terrier 36 53.0%\n",
      "Yorkshire Terrier 37 50.0%\n",
      "12345678910111213141516171819202122232425262728293031323334353637\n",
      "Figure6.Confusionmatrixforbreeddiscrimination. Thever-\n",
      "ticalaxisreportsthegroundtruthlabels,andthehorizontalaxisto\n",
      "thepredictedones(theupper-leftblockarethecats).Thematrixis\n",
      "normalizedbyrowandthevaluesalongthediagonalarereported\n",
      "ontheright. Thematrixcorrespondstothebreedclassiﬁerusing\n",
      "shape features, appearance features with the image, head, body,\n",
      "body-head layouts with automatic segmentations, and a 37-class\n",
      "SVM. This is the best result for breed classiﬁcation, and corre-\n",
      "spondstothelastentryofrownumber8inTab.4.\n",
      "a b c d\n",
      "Figure 5. Example segmentation results on Oxford-IIIT Pet\n",
      "dataset. The segmentation of the pet from the background was\n",
      "obtainedautomaticallyasdescribedinSect.3.3. e f g h\n",
      "Figure7.Failurecasesforthemodelusingappearanceonly(im-\n",
      "age layout) in Sect. 4.2. First row: Cat images that were incor-\n",
      "ﬁcation,inwhicha37-classSVMislearneddirectly,using rectlyclassiﬁedasdogsandviceversa. Secondrow: Bengalcats\n",
      "the same method discussed in Sect. 4.2. The relative per- (b–d) classiﬁed as Egyptian Mau (a). Third row: English Setter\n",
      "formanceofthedifferentmodelsissimilartothatobserved (f–h)classiﬁedasEnglishCockerSpaniel(e).\n",
      "in Sect. 4.1 and 4.2. Flat classiﬁcation is better than hier-\n",
      "archical, but the latter requires less work at test time, due\n",
      "5.Summary\n",
      "tothefactthatfewerSVMclassiﬁersneedtobeevaluated.\n",
      "For example, using the appearance model with the image, This paper has introduced the Oxford-IIIT Pet dataset\n",
      "head, image-head layouts for 37 class classiﬁcation yields for the ﬁne-grained categorisation problem of identifying\n",
      "an accuracy of 51.23%, adding the shape information hi- thefamilyandbreedofpets(catsanddogs). Threediffer-\n",
      "erarchically improves this accuracy to 52.78%, and using enttasksandcorrespondingbaselinealgorithmshavebeen\n",
      "shape and appearance together in a ﬂat classiﬁcation ap- proposedandinvestigatedobtainingveryencouragingclas-\n",
      "proachachievesanaccuracy54.03%. Theconfusionmatrix siﬁcation results on the dataset. Furthermore, the baseline\n",
      "forthebestresultforbreedclassiﬁcation,correspondingto modelswereshowntoachievestate-of-the-artperformance\n",
      "thelastentryoftheeightrowofTable4isshowninFig.4. ontheASIRRAchallengedata,breakingthetestwith42%\n",
      "probability,aremarkableachievementconsideringthatthis [24] F.FleuretandD.Geman. Stationaryfeaturesandcatdetec-\n",
      "datasetwasdesignedtobechallengingformachines. tion. JournalofMachineLearningResearch,9,2008.\n",
      "[25] P.Golle.Machinelearningattacksagainsttheasirracaptcha.\n",
      "In15thACMConferenceonComputerandCommunications\n",
      "Acknowledgements. We are grateful for ﬁnancial sup- Security(CCS),2008.\n",
      "[26] G.Grifﬁn,A.Holub,andP.Perona. Caltech-256objectcat-\n",
      "portfromtheUKIERI,EUProjectAXESICT-269980and\n",
      "egorydataset. Technicalreport,CaliforniaInstituteofTech-\n",
      "ERCgrantVisRecno. 228180.\n",
      "nology,2007.\n",
      "[27] A.Khosla,N.Jayadevaprakash,B.Yao,andF.F.Li. Novel\n",
      "References datasetforﬁne-grainedimagecategorization. InFirstWork-\n",
      "shoponFine-GrainedVisualCategorization,CVPR,2011.\n",
      "[1] Americankennelclub. http://www.akc.org/. [28] C.Lampert,H.Nickisch,andS.Harmeling. Learningtode-\n",
      "[2] The cat fanciers association inc. http://www.cfa.\n",
      "tectunseenobjectclassesbybetween-classattributetransfer.\n",
      "org/Client/home.aspx.\n",
      "InProc.CVPR,2009.\n",
      "[3] Catsinsinks. http://catsinsinks.com/.\n",
      "[29] I.Laptev. Improvements of object detection usingboosted\n",
      "[4] Catster. http://www.catster.com/.\n",
      "histograms. InProc.BMVC,2006.\n",
      "[5] Dogster. http://www.dogster.com/.\n",
      "[30] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bag of\n",
      "[6] Flickr! http://www.flickr.com/.\n",
      "features: Spatial pyramid matching for recognizing natural\n",
      "[7] Googleimages. http://images.google.com/.\n",
      "[8] The international cat association. http://www.tica. scenecategories. InProc.CVPR,2006.\n",
      "[31] D. G. Lowe. Object recognition from local scale-invariant\n",
      "org/.\n",
      "[9] Mycatspace. http://www.mycatspace.com/. features. InProc.ICCV,1999.\n",
      "[10] Mydogspace. http://www.mydogspace.com/. [32] M.-E.NilsbackandA.Zisserman. Avisualvocabularyfor\n",
      "[11] Petﬁnder. http://www.petfinder.com/index. ﬂowerclassiﬁcation. InProc.CVPR,2006.\n",
      "html. [33] M.-E.NilsbackandA.Zisserman. Automatedﬂowerclas-\n",
      "[12] Worldcanineorganisation. http://www.fci.be/. siﬁcationoveralargenumberofclasses. InProc.ICVGIP,\n",
      "[13] P.Arbelaez,M.Maire,C.Fowlkes,andJ.Malik. Fromcon- 2008.\n",
      "tourstoregions: Anempiricalevaluation. InProc.CVPR, [34] O.Parkhi,A.Vedaldi,C.V.Jawahar,andA.Zisserman.The\n",
      "2009. truthaboutcatsanddogs. InProc.ICCV,2011.\n",
      "[14] S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder, [35] O.Parkhi,A.Vedaldi,A.Zisserman,andC.V.Jawahar.The\n",
      "P.Perona,andS.Belongie. Visualrecognitionwithhumans Oxford-IIIT PET Dataset. http://www.robots.ox.\n",
      "intheloop. InProc.ECCV,2010. ac.uk/˜vgg/data/pets/index.html,2012.\n",
      "[15] Y.Chai,V.Lempitsky,andA.Zisserman. Bicos: Abi-level [36] C.Rother, V.Kolmogorov, andA.Blake. “grabcut”—in-\n",
      "co-segmentation method for image classiﬁcation. In Proc. teractiveforegroundextractionusingiteratedgraphcuts. In\n",
      "ICCV,2011. ACMTrans.onGraphics,2004.\n",
      "[16] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and [37] B.Scho¨lkopfandA.J.Smola. LearningwithKernels. MIT\n",
      "C. Bray. Visual categorization with bags of keypoints. In Press,2002.\n",
      "[38] J. Sivic and A. Zisserman. Video Google: A text retrieval\n",
      "Proc. ECCV Workshop on Stat. Learn. in Comp. Vision,\n",
      "approachtoobjectmatchinginvideos.InProc.ICCV,2003.\n",
      "2004.\n",
      "[39] M.VarmaandD.Ray. Learningthediscriminativepower-\n",
      "[17] N.DalalandB.Triggs. Histogramsoforientedgradientsfor\n",
      "invariancetrade-off. InProc.ICCV,2007.\n",
      "humandetection. InProc.CVPR,2005.\n",
      "[40] A. Vedaldi and B. Fulkerson. VLFeat library. http://\n",
      "[18] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.\n",
      "www.vlfeat.org/,2008.\n",
      "ImageNet: ALarge-ScaleHierarchicalImageDatabase. In\n",
      "[41] A.Vedaldi,V.Gulshan,M.Varma,andA.Zisserman. Mul-\n",
      "Proc.CVPR,2009.\n",
      "tiplekernelsforobjectdetection. InProc.ICCV,2009.\n",
      "[19] J. Elson, J. Douceur, J. Howell, and J. J. Saul. Asirra: A\n",
      "[42] A.VedaldiandA.Zisserman. Structuredoutputregression\n",
      "CAPTCHAthatexploitsinterest-alignedmanualimagecat-\n",
      "fordetectionwithpartialocculsion. InProc.NIPS,2009.\n",
      "egorization. InConf.onComputerandCommunicationsSe-\n",
      "[43] P. Welinder, S. Branson, T. Mita, C. Wah, and F. Schroff.\n",
      "curity(CCS),2007.\n",
      "Caltech-ucsd birds 200. Technical report, Caltech-UCSD,\n",
      "[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\n",
      "2010.\n",
      "and A. Zisserman. The PASCAL Visual Object Classes\n",
      "[44] J.Zhang,M.Marszalek,S.Lazebnik,andC.Schmid. Local\n",
      "Challenge 2011 (VOC2011) Results. http://www.pascal-\n",
      "features and kernels for classiﬁcation of texture and object\n",
      "network.org/challenges/VOC/voc2011/workshop/index.html.\n",
      "categories:Acomprehensivestudy. IJCV,2007.\n",
      "[21] R.-E.Fan,K.-W.Chang,C.-J.Hsieh,X.-R.Wang,andC.-J.\n",
      "[45] W. Zhang, J. Sun, and X. Tang. Cat head detection - how\n",
      "Lin. LIBLINEAR: A library for large linear classiﬁcation.\n",
      "to effectively exploit shape and texture features. In Proc.\n",
      "JournalofMachineLearningResearch,9,2008.\n",
      "ECCV,2008.\n",
      "[22] L.Fei-Fei,R.Fergus,andP.Perona.ABayesianapproachto\n",
      "unsupervisedone-shotlearningofobjectcategories.InProc.\n",
      "ICCV,2003.\n",
      "[23] P.F.Felzenszwalb,R.B.Grishick,D.McAllester,andD.Ra-\n",
      "manan. Object detection with discriminatively trained part\n",
      "basedmodels. PAMI,2009.\n"
     ]
    }
   ],
   "source": [
    "## 2. 提取所有页pdf文字」\n",
    "\n",
    "import pdfplumber\n",
    "with pdfplumber.open(\"example.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()#提取文本\n",
    "        print(text)\n",
    "## 3. 「提取所有pdf文字并写入文本中」\n",
    "\n",
    "import pdfplumber\n",
    "with pdfplumber.open(\"example.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()#提取文本\n",
    "        txt_file = open(\"example.txt\",mode='a',encoding='utf-8')\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[[['Breed', 'Training', 'Validation', 'Test', 'Total', 'Breed', 'Training', 'Validation', 'Test', 'Total'], ['Abyssinian\\nBengal\\nBirman\\nBombay\\nBritishShorthair\\nEgyptianMau\\nMaineCoon\\nPersian\\nRagdoll\\nRussianBlue\\nSiamese\\nSphynx', '50\\n50\\n50\\n49\\n50\\n47\\n50\\n50\\n50\\n50\\n50\\n50', '50\\n50\\n50\\n47\\n50\\n46\\n50\\n50\\n50\\n50\\n49\\n50', '98\\n100\\n100\\n88\\n100\\n97\\n100\\n100\\n100\\n100\\n100\\n100', '198\\n200\\n200\\n184\\n200\\n190\\n200\\n200\\n200\\n200\\n199\\n200', 'EnglishSetter\\nGermanShorthaired\\nGreatPyrenees\\nHavanese\\nJapaneseChin\\nKeeshond\\nLeonberger\\nMiniaturePinscher\\nNewfoundland\\nPomeranian\\nPug\\nSaintBernard\\nSamoyed\\nScottishTerrier\\nShibaInu\\nStaffordshireBullTerrier\\nWheatenTerrier\\nYorkshireTerrier', '50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50', '50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n46\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50\\n50', '100\\n100\\n100\\n100\\n100\\n99\\n100\\n100\\n100\\n100\\n100\\n100\\n100\\n99\\n100\\n89\\n100\\n100', '200\\n200\\n200\\n200\\n200\\n199\\n200\\n200\\n196\\n200\\n200\\n200\\n200\\n199\\n200\\n189\\n200\\n200'], ['AmericanBulldog\\nAmericanPitBullTerrier\\nBassetHound\\nBeagle\\nBoxer\\nChihuahua\\nEnglishCockerSpaniel', '50\\n50\\n50\\n50\\n50\\n50\\n50', '50\\n50\\n50\\n50\\n50\\n50\\n46', '100\\n100\\n100\\n100\\n99\\n100\\n100', '200\\n200\\n200\\n200\\n199\\n200\\n196', None, None, None, None, None], [None, None, None, None, None, 'Total', '1846', '1834', '3669', '7349']]]\n",
      "[]\n",
      "[[['Method', 'MeanSegmentationAccuracy'], ['Allforeground', '45%'], ['Parkhietal.[34]', '61%'], ['Thispaper', '65%']], [['Dataset', 'MeanClassiﬁcationAccuracy'], ['Oxford-IIITPetDataset', '38.45%'], ['UCSD-CaltechBirds', '6.91%'], ['Oxford-Flowers102', '53.71%']]]\n",
      "[[['.', 'Shape', 'Appearance', None, 'ClassiﬁcationAccuracy(%)', None, None], ['', '', 'layouttype', 'usinggroundtruth', 'family', 'breed(S.4.2)', 'both(S.4.3)'], ['', '', '', '', '(S.4.1)', 'cat dog', 'hierarchical ﬂat'], ['1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9', '(cid:88)\\n–\\n–\\n–\\n–\\n(cid:88)\\n(cid:88)\\n(cid:88)\\n(cid:88)', '–\\nImage\\nImage+Head\\nImage+Head+Body\\nImage+Head+Body\\nImage\\nImage+Head\\nImage+Head+Body\\nImage+Head+Body', '–\\n–\\n–\\n–\\n(cid:88)\\n–\\n–\\n–\\n(cid:88)', '94.21\\n82.56\\n85.06\\n87.78\\n88.68\\n94.88\\n95.07\\n94.89\\n95.37', 'NA NA\\n52.01 40.59\\n60.37 52.10\\n64.27 54.31\\n66.12 57.29\\n50.27 42.94\\n59.11 54.56\\n63.48 55.68\\n66.07 59.18', 'NA NA\\nNA 39.64\\nNA 51.23\\nNA 54.05\\nNA 56.60\\n42.29 43.30\\n52.78 54.03\\n55.26 56.68\\n57.77 59.21']], [['Method', 'MeanClass.Accuracy'], ['Golleetal.[25]', '82.7%'], ['Thispaper(Shapeonly)', '92.9%']]]\n",
      "[[['', ''], ['', '']]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## 4. 提取PDF表格\n",
    "## 「提取表格」\n",
    "\n",
    "## 提取pdf表格\n",
    "import pdfplumber\n",
    "with pdfplumber.open(\"example.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # page01 = pdf.pages[0] #指定页码\n",
    "        # table1 = page01.extract_table()#提取单个表格\n",
    "        table = page.extract_tables()#提取多个表格\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m workbook \u001b[39m=\u001b[39m Workbook()\n\u001b[0;32m     10\u001b[0m sheet \u001b[39m=\u001b[39m workbook\u001b[39m.\u001b[39mactive\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m table:\n\u001b[0;32m     12\u001b[0m     sheet\u001b[39m.\u001b[39mappend(row)\n\u001b[0;32m     13\u001b[0m workbook\u001b[39m.\u001b[39msave(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtable_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.xlsx\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "## 「提取表格，保存为excel文件」\n",
    "\n",
    "import pdfplumber\n",
    "from openpyxl import Workbook #保存表格，需要安装openpyxl\n",
    "with pdfplumber.open(\"example.pdf\") as pdf:\n",
    "    i = 0\n",
    "    for page in pdf.pages:\n",
    "        table = page.extract_table()#提取多个表格\n",
    "        workbook = Workbook()\n",
    "        sheet = workbook.active\n",
    "        for row in table:\n",
    "            sheet.append(row)\n",
    "        workbook.save(filename=\"table_{}.xlsx\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pypdf2部分：\n",
    "## 1. 分割PDF\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "file_reader = PdfFileReader(\"example.pdf\")\n",
    "# getNumPages() 获取总页数\n",
    "for page in range(file_reader.getNumPages()):\n",
    "    # 实例化对象\n",
    "    file_writer = PdfFileWriter()\n",
    "    # 将遍历的每一页添加到实例化对象中\n",
    "    file_writer.addPage(file_reader.getPage(page))\n",
    "    with open(\"./pdf/{}.pdf\".format(page),'wb') as out:\n",
    "        file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. 合并PDF\n",
    "### 将上述分割的pdf合并成一个文件\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "file_writer = PdfFileWriter()\n",
    "for page in range(8):\n",
    "    # 循环读取需要合并pdf文件\n",
    "    file_reader = PdfFileReader(\"./pdf/{}.pdf\".format(page))\n",
    "    # 遍历每个pdf的每一页\n",
    "    for page in range(file_reader.getNumPages()):\n",
    "        # 写入实例化对象中\n",
    "        file_writer.addPage(file_reader.getPage(page))\n",
    "\n",
    "with open(\"./pdf/合并.pdf\",'wb') as out:\n",
    "    file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. PDF旋转\n",
    "# 旋转pdf，只能按照90度的倍数旋转\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "file_reader = PdfFileReader(\"example.pdf\")\n",
    "file_writer = PdfFileWriter()\n",
    "page = file_reader.getPage(0).rotateClockwise(90) # 第1页顺时针旋转90度\n",
    "file_writer.addPage(page) # 写入\n",
    "page = file_reader.getPage(1).rotateCounterClockwise(90) # 第2页逆时针旋转90度\n",
    "file_writer.addPage(page) # 写入\n",
    "with open(\"旋转.pdf\",'wb') as out:\n",
    "    file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. PDF加密解密\n",
    "### 「添加密码」\n",
    "\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "file_reader = PdfFileReader(\"example.pdf\")\n",
    "file_writer = PdfFileWriter()\n",
    "for page in range(file_reader.getNumPages()):\n",
    "    file_writer.addPage(file_reader.getPage(page))\n",
    "\n",
    "file_writer.encrypt('123456') # 设置密码\n",
    "with open(\"加密后.pdf\",'wb') as out:\n",
    "    file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 「解除密码」\n",
    "\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "file_reader = PdfFileReader(\"加密后.pdf\")\n",
    "file_reader.decrypt('123456')\n",
    "file_writer = PdfFileWriter()\n",
    "for page in range(file_reader.getNumPages()):\n",
    "    file_writer.addPage(file_reader.getPage(page))\n",
    "\n",
    "with open(\"解密后.pdf\",'wb') as out:\n",
    "    file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'7' b'0'\n",
      "Superfluous whitespace found in object header b'8' b'0'\n",
      "Superfluous whitespace found in object header b'6' b'0'\n",
      "Superfluous whitespace found in object header b'5' b'0'\n",
      "Superfluous whitespace found in object header b'4' b'0'\n"
     ]
    }
   ],
   "source": [
    "## 5. PDF添加水印\n",
    "### 首先准备一个水印文档，可以用空白word添加图片或者文字转成pdf文件。\n",
    "# 添加水印\n",
    "from PyPDF2 import  PdfFileReader, PdfFileWriter\n",
    "from copy import copy\n",
    "sy = PdfFileReader(\"水印.pdf\")\n",
    "mark_page = sy.getPage(0) # 水印所在的页数\n",
    "# 读取添加水印的文件\n",
    "file_reader = PdfFileReader(\"example.pdf\")\n",
    "file_writer = PdfFileWriter()\n",
    "\n",
    "for page in range(file_reader.getNumPages()):\n",
    "    # 读取需要添加水印每一页pdf\n",
    "    source_page = file_reader.getPage(page)\n",
    "    new_page = copy(mark_page) #\n",
    "    new_page.mergePage(source_page) # new_page(水印)在下面，source_page原文在上面\n",
    "    file_writer.addPage(new_page)\n",
    "\n",
    "with open(\"添加水印后.pdf\",'wb') as out:\n",
    "    file_writer.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "seek of closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m pdf\u001b[39m.\u001b[39mpages:\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mfor\u001b[39;00m image_file_object \u001b[39min\u001b[39;00m page\u001b[39m.\u001b[39;49mimages:\n\u001b[0;32m     10\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./figure/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(count) \u001b[39m+\u001b[39m image_file_object\u001b[39m.\u001b[39mname, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m     11\u001b[0m             fp\u001b[39m.\u001b[39mwrite(image_file_object\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfplumber\\container.py:46\u001b[0m, in \u001b[0;36mContainer.images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimages\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T_obj_list:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjects\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, [])\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfplumber\\page.py:196\u001b[0m, in \u001b[0;36mPage.objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_objects\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects\n\u001b[1;32m--> 196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects: Dict[\u001b[39mstr\u001b[39m, T_obj_list] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_objects()\n\u001b[0;32m    197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfplumber\\page.py:254\u001b[0m, in \u001b[0;36mPage.parse_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_objects\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, T_obj_list]:\n\u001b[0;32m    253\u001b[0m     objects: Dict[\u001b[39mstr\u001b[39m, T_obj_list] \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_layout_objects(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayout\u001b[39m.\u001b[39m_objs):\n\u001b[0;32m    255\u001b[0m         kind \u001b[39m=\u001b[39m obj[\u001b[39m\"\u001b[39m\u001b[39mobject_type\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m kind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39manno\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfplumber\\page.py:142\u001b[0m, in \u001b[0;36mPage.layout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m device \u001b[39m=\u001b[39m PDFPageAggregator(\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mrsrcmgr,\n\u001b[0;32m    138\u001b[0m     pageno\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpage_number,\n\u001b[0;32m    139\u001b[0m     laparams\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mlaparams,\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m interpreter \u001b[39m=\u001b[39m PDFPageInterpreter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mrsrcmgr, device)\n\u001b[1;32m--> 142\u001b[0m interpreter\u001b[39m.\u001b[39;49mprocess_page(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpage_obj)\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout: LTPage \u001b[39m=\u001b[39m device\u001b[39m.\u001b[39mget_result()\n\u001b[0;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfinterp.py:997\u001b[0m, in \u001b[0;36mPDFPageInterpreter.process_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    995\u001b[0m     ctm \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39mx0, \u001b[39m-\u001b[39my0)\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mbegin_page(page, ctm)\n\u001b[1;32m--> 997\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_contents(page\u001b[39m.\u001b[39;49mresources, page\u001b[39m.\u001b[39;49mcontents, ctm\u001b[39m=\u001b[39;49mctm)\n\u001b[0;32m    998\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mend_page(page)\n\u001b[0;32m    999\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfinterp.py:1014\u001b[0m, in \u001b[0;36mPDFPageInterpreter.render_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[39m\"\"\"Render the content streams.\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \n\u001b[0;32m   1009\u001b[0m \u001b[39mThis method may be called recursively.\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m   1012\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrender_contents: resources=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m, streams=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m, ctm=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, resources, streams, ctm\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_resources(resources)\n\u001b[0;32m   1015\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_state(ctm)\n\u001b[0;32m   1016\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute(list_value(streams))\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfinterp.py:384\u001b[0m, in \u001b[0;36mPDFPageInterpreter.init_resources\u001b[1;34m(self, resources)\u001b[0m\n\u001b[0;32m    382\u001b[0m             objid \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mobjid\n\u001b[0;32m    383\u001b[0m         spec \u001b[39m=\u001b[39m dict_value(spec)\n\u001b[1;32m--> 384\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmap[fontid] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrsrcmgr\u001b[39m.\u001b[39;49mget_font(objid, spec)\n\u001b[0;32m    385\u001b[0m \u001b[39melif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColorSpace\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    386\u001b[0m     \u001b[39mfor\u001b[39;00m (csid, spec) \u001b[39min\u001b[39;00m dict_value(v)\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfinterp.py:216\u001b[0m, in \u001b[0;36mPDFResourceManager.get_font\u001b[1;34m(self, objid, spec)\u001b[0m\n\u001b[0;32m    213\u001b[0m     subtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mType1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m subtype \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mType1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMMType1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    215\u001b[0m     \u001b[39m# Type1 Font\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     font \u001b[39m=\u001b[39m PDFType1Font(\u001b[39mself\u001b[39;49m, spec)\n\u001b[0;32m    217\u001b[0m \u001b[39melif\u001b[39;00m subtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTrueType\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[39m# TrueType Font\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     font \u001b[39m=\u001b[39m PDFTrueTypeFont(\u001b[39mself\u001b[39m, spec)\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdffont.py:1002\u001b[0m, in \u001b[0;36mPDFType1Font.__init__\u001b[1;34m(self, rsrcmgr, spec)\u001b[0m\n\u001b[0;32m    999\u001b[0m PDFSimpleFont\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, descriptor, widths, spec)\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mEncoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m spec \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mFontFile\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m descriptor:\n\u001b[0;32m   1001\u001b[0m     \u001b[39m# try to recover the missing encoding info from the font file.\u001b[39;00m\n\u001b[1;32m-> 1002\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontfile \u001b[39m=\u001b[39m stream_value(descriptor\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mFontFile\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m   1003\u001b[0m     length1 \u001b[39m=\u001b[39m int_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontfile[\u001b[39m\"\u001b[39m\u001b[39mLength1\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1004\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontfile\u001b[39m.\u001b[39mget_data()[:length1]\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdftypes.py:217\u001b[0m, in \u001b[0;36mstream_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream_value\u001b[39m(x: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPDFStream\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m     x \u001b[39m=\u001b[39m resolve1(x)\n\u001b[0;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, PDFStream):\n\u001b[0;32m    219\u001b[0m         \u001b[39mif\u001b[39;00m settings\u001b[39m.\u001b[39mSTRICT:\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdftypes.py:118\u001b[0m, in \u001b[0;36mresolve1\u001b[1;34m(x, default)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m\"\"\"Resolves an object.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[39mIf this is an array or dictionary, it may still contains\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39msome indirect objects inside.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39misinstance\u001b[39m(x, PDFObjRef):\n\u001b[1;32m--> 118\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mresolve(default\u001b[39m=\u001b[39;49mdefault)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdftypes.py:106\u001b[0m, in \u001b[0;36mPDFObjRef.resolve\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc\u001b[39m.\u001b[39;49mgetobj(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjid)\n\u001b[0;32m    107\u001b[0m \u001b[39mexcept\u001b[39;00m PDFObjectNotFound:\n\u001b[0;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfdocument.py:866\u001b[0m, in \u001b[0;36mPDFDocument.getobj\u001b[1;34m(self, objid)\u001b[0m\n\u001b[0;32m    864\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getobj_objstm(stream, index, objid)\n\u001b[0;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 866\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getobj_parse(index, objid)\n\u001b[0;32m    867\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecipher:\n\u001b[0;32m    868\u001b[0m         obj \u001b[39m=\u001b[39m decipher_all(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecipher, objid, genno, obj)\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\pdfdocument.py:818\u001b[0m, in \u001b[0;36mPDFDocument._getobj_parse\u001b[1;34m(self, pos, objid)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getobj_parse\u001b[39m(\u001b[39mself\u001b[39m, pos: \u001b[39mint\u001b[39m, objid: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    817\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parser\u001b[39m.\u001b[39;49mseek(pos)\n\u001b[0;32m    819\u001b[0m     (_, objid1) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser\u001b[39m.\u001b[39mnexttoken()  \u001b[39m# objid\u001b[39;00m\n\u001b[0;32m    820\u001b[0m     (_, genno) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser\u001b[39m.\u001b[39mnexttoken()  \u001b[39m# genno\u001b[39;00m\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\psparser.py:557\u001b[0m, in \u001b[0;36mPSStackParser.seek\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseek\u001b[39m(\u001b[39mself\u001b[39m, pos: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 557\u001b[0m     PSBaseParser\u001b[39m.\u001b[39;49mseek(\u001b[39mself\u001b[39;49m, pos)\n\u001b[0;32m    558\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    559\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mi:\\TGY-being\\project\\python\\pypdf2_env\\lib\\site-packages\\pdfminer\\psparser.py:220\u001b[0m, in \u001b[0;36mPSBaseParser.seek\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"Seeks the parser to the given position.\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mseek: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, pos)\n\u001b[1;32m--> 220\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mseek(pos)\n\u001b[0;32m    221\u001b[0m \u001b[39m# reset the status for nextline()\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbufpos \u001b[39m=\u001b[39m pos\n",
      "\u001b[1;31mValueError\u001b[0m: seek of closed file"
     ]
    }
   ],
   "source": [
    "## 6. 提取图像\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"example.pdf\")\n",
    "count = 0\n",
    "for page in pdf.pages:\n",
    "    for image_file_object in page.images:\n",
    "        with open('./figure/' + str(count) + image_file_object.name, \"wb\") as fp:\n",
    "            fp.write(image_file_object.data)\n",
    "            count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pypdf2_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcef7f9d12438fefc053a63ee9479c1c9cca8e6f73d04ae8bedf059c85ce9bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
